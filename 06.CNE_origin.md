# Align CNE to different genomes（python）
``` python
#! /usr/bin/env python
import re, os
import datetime
from multiprocessing import Pool

def run_blastn(command):
    os.system(command)

def process_alignment(ref_fa_dir, query_fa, out_dir):
    makeblastdb = '/data01/wangkun/software/blast/ncbi-blast-2.15.0+/bin/makeblastdb'
    blastn = '/data01/wangkun/software/blast/ncbi-blast-2.15.0+/bin/blastn'
    tblastn = '/data01/wangkun/software/blast/ncbi-blast-2.15.0+/bin/tblastn'
    commands = []
    for filename in os.listdir(ref_fa_dir):
        if filename.endswith('.fa'):
            file_path = os.path.join(ref_fa_dir, filename)
            pattern = re.match('(\S+).fa', filename)
            species_name = pattern.group(1)
            outfile = f'{out_dir}/{species_name}.out'
            if os.path.exists(outfile): continue
            print(f'{makeblastdb} -in {file_path} -dbtype nucl -out {file_path}')
            command = f'{blastn} -query {query_fa} -db {file_path} -out {out_dir}/{species_name}.out -outfmt 6 -num_threads 32 -evalue 1e-5 -max_target_seqs 6 -max_hsps 3 -task dc-megablast -template_length 16 -template_type optimal'
            commands.append(command)
    

ref_fa_dir = 'target_genomes_2'
query_fa = 'allgene_loc.pep'
out_dir = 'blast_out_step2'

process_alignment(ref_fa_dir, query_fa, out_dir)
```
# Filter blast results
``` shell
python3 ./Blastn_picker_multi_Q.py -q long.fasta -m ./blast_out_step2/BBshark.out -e 1e-5 -o ./bed/BBshark
```
``` python
#Blastn_picker_multi_Q.py
#! /usr/bin/env python3
import re, sys, os
import argparse

def merge(intervals, connector_len):
    """best merge funciton found
    Args:
        intervals (list of tuple or list of list): [(1,2), (4,7), (8,10), (12,16)]

    Returns:
        list of list: list of list of merged intervals
    """
    if connector_len == 0:
        sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])
        merged = []
        for higher in sorted_by_lower_bound:
            if not merged:
                merged.append(higher)
            else:
                lower = merged[-1]
                expansion = max(higher[1] - higher[0], lower[1] - lower[0])
                # test for intersection between lower and higher:
                # we know via sorting that lower[0] <= higher[0]
                if higher[0] - expansion <= lower[1]:
                    upper_bound = max(lower[1], higher[1])
                    # replace by merged interval
                    merged[-1] = (lower[0], upper_bound)
                else:
                    merged.append(higher)
        return merged
    elif connector_len:
        sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])
        merged = []
        for higher in sorted_by_lower_bound:
            if not merged:
                merged.append(higher)
            else:
                lower = merged[-1]
                expansion = max(higher[1] - higher[0], lower[1] - lower[0], connector_len)
                # test for intersection between lower and higher:
                # we know via sorting that lower[0] <= higher[0]
                if higher[0] - expansion <= lower[1]:
                    upper_bound = max(lower[1], higher[1])
                    merged[-1] = (lower[0], upper_bound)  # replace by merged interval
                else:
                    merged.append(higher)
    return merged


# combine effective ranges among tblastn results
def combine(hit_list, gene_interval):
    """
    combine effective ranges among tblastn results
    :param list:
    :return:
    """
    list = []
    output = []
    hit_list = sorted(hit_list, key=lambda x: int(x[0]))
    for item in hit_list:
        list.append([int(item[0]), int(item[1])])
    seed = list[0]
    for item in list:
        if float(min(item) - max(seed)) < gene_interval and float(min(seed) - max(item)) < gene_interval:
            if seed[0] < seed[1]:
                seed[0] = min(seed[0], item[0])
                seed[1] = max(seed[1], item[1])
            else:
                seed[0] = max(seed[0], item[0])
                seed[1] = min(seed[1], item[1])
            if item == list[-1]:
                output.append(seed)
                break
        else:
            output.append(seed)
            seed = item
    return output


def main():
    # use parser to add parameter: m8_output
    parser = argparse.ArgumentParser(description="pickup Blastn m8 result and build a bed file in the meantime")
    parser.add_argument("-q", "--query_fasta", help="query_fasta_file", required=True)
    parser.add_argument("-m", "--m8_output", help="m8_output", required=True)
    parser.add_argument("-e", "--evalue", help="evalue", required=True)
    parser.add_argument("-l", "--connector_len", help="connector_len", default=200, required=False)
    parser.add_argument("-o", "--output_prefix", help="output_prefix", required=True)
    parser.add_argument("-d", "--element_delimiter", help="element delimiter", default=":", required=False)
    parser.add_argument("-a", "--max_possible_hits", help="max possible number of hits", default=4, required=False)
    args = parser.parse_args()
    m8_output = args.m8_output

    # check if the output file exists
    if not os.path.exists(m8_output):
        print("ERROR: m8_output file not exists")
        sys.exit(1)

    # parse and filter blastn m8 output
    # hit_map will be {'Query1':[[scaf, start, end], [scaf, start, end], ...], 'Query2':[[scaf, start, end], [scaf, start, end], ...]}
    hit_map = {}
    print('building hit_map...')
    with open(m8_output) as m8:
        for line in m8:
            if line.startswith("#"):
                continue
            else:
                info = line.strip().split("\t")
                query_len = int(re.search(r'\S+\:(\d+)-(\d+)', info[0]).group(2)) - int(re.search(r'\S+\:(\d+)-(\d+)', info[0]).group(1))
                query_start = int(info[6])
                query_end = int(info[7])
                ref_start = int(info[8])
                ref_end = int(info[9])
                evalue = float(info[10])
                if evalue > float(args.evalue):
                    continue
                # remove REPEATS
                if abs(ref_start - ref_end) < 30:
                    # alternative query_len * 0.1
                    continue
                elif ref_end > ref_start:
                    hit_map.setdefault(info[0], {}).setdefault(info[1]+'+', []).append([ref_start, ref_end])
                elif ref_end < ref_start:
                    hit_map.setdefault(info[0], {}).setdefault(info[1]+'-', []).append([ref_end, ref_start])


    # combine effective ranges among tblastn results
    print('picking up effective hits...')
    effective_hit_map = {}
    # read query fasta file to get query length
    query_fasta = args.query_fasta
    length_info = {}
    with open(query_fasta) as fasta:
        for line in fasta:
            if line.startswith(">"):
                query_name = line.strip().lstrip('>')
                query_s, query_e, query_alias = re.search(r'\S+\:(\d+)\-(\d+):(.*)', query_name).groups()
                query_len = int(query_e) - int(query_s)
                if query_alias not in length_info:
                    length_info[query_alias] = query_len
                elif query_len > length_info[query_alias]:
                    length_info[query_alias] = query_len

    # key1 in queryname
    for key1 in hit_map:
        hits = hit_map[key1]
        query_len = int(re.search(r'\S+\:(\d+)-(\d+)', key1).group(2)) - int(re.search(r'\S+\:(\d+)-(\d+)', key1).group(1))
        # key2 is scaffold name
        for key2 in hits:
            # reverse sort by length
            # query_len used as connector_len here
            merged_intervals = []
            connector_len = int(args.connector_len)
            merged_intervals = sorted(list(merge([item for item in hits[key2]], connector_len)), key=lambda x: x[1] - x[0], reverse=True)
            if len(merged_intervals) > args.max_possible_hits or merged_intervals[0][1] - merged_intervals[0][0] < query_len * 0.2:
                continue
            elif merged_intervals[0][1] - merged_intervals[0][0] > query_len * 5:
                continue
            effective_hit_map.setdefault(key1, {}).setdefault(key2, merged_intervals[0])
        if key1 not in effective_hit_map:
            continue
        # only maintain the best hit for each query
        best_hit = [0, 0, 0]
        for key2 in effective_hit_map[key1]:
            start = effective_hit_map[key1][key2][0]
            end = effective_hit_map[key1][key2][1]
            if end-start > best_hit[2] - best_hit[1]:
                best_hit = [key2, start, end]
        effective_hit_map[key1] = best_hit

    multi_effective_hit_map = {}
    for key1 in effective_hit_map:
        tag = key1.split(args.element_delimiter)[-1]
        hit = effective_hit_map[key1]
        if hit[0] == 0:
            continue
        multi_effective_hit_map.setdefault(tag, []).append(hit)
        
    for key1 in list(multi_effective_hit_map.keys()):
        hits = multi_effective_hit_map[key1]
        if len(hits) == 1:
            multi_effective_hit_map[key1] = hits[0]
            continue
        else:
            combined_hit = merge([item[1:] for item in hits], 0)
            if len(combined_hit) > 1:
                del multi_effective_hit_map[key1]
                continue
            else:
                multi_effective_hit_map[key1] = [hits[0][0], combined_hit[0][0], combined_hit[0][1]]

    # build bed file for comparison with previous db
    print('Writing outputs...')
    with open(args.output_prefix + ".bed", "w") as bed:
        for key in multi_effective_hit_map:
            hit = multi_effective_hit_map[key]
            best_scaf, best_start, best_end = hit
            bed.write("\t".join([best_scaf.rstrip('+|-'), str(best_start), str(best_end)]) + "\n")
    with open(args.output_prefix + ".out", 'w') as out:
        for key in multi_effective_hit_map:
            mean_len = length_info[key]
            query_info = f"{key}:1-{mean_len}"
            hit = multi_effective_hit_map[key]
            best_scaf, best_start, best_end = hit
            out.write("\t".join([query_info, best_scaf.rstrip('+|-'), str(best_start), str(best_end), re.search(r'([+-])$', best_scaf)[0]]) + "\n")


if __name__ == '__main__':
    main()
```
# Obtain enhancer-CNE (shell)
``` shell
bedtools intersect -wb -a /data02/liye/project/analys_sr/11.qiyuan/05.enhancer_orgin/mouse_enhancer/all.sort.merge.peak.sort.bed -b /data02/liye/project/analys_sr/24.gene_origin/04.cne/bed/mouse.out.bed.sort> mouse_active.cne
bedtools intersect -wb -a 03.enhancer_cne/02.chicken_cne/test.sort.merge.bed -b /data02/liye/project/analys_sr/24.gene_origin/04.cne/bed/chicken.out.out.bed.sort > chicken_active.cne
```

# Statistical Coverage
``` shell
python3 04.stat.py
``` 
```python 
##04.stat.py
#! /usr/bin/env python3
import os
from glob import glob
from posixpath import basename
import re
from matplotlib.pyplot import ylabel
import pandas as pd
import seaborn as sns


def coverage(line: str) -> float:
    """
    get coverage of a line
    :param line: a line of output
    :return: coverage
    """
    query = line.split('\t')[0]
    query_len = int(re.search(r'\S+:(\d+)-(\d+)', query).group(2)) - int(re.search(r'\S+:(\d+)-(\d+)', query).group(1))
    ref_len = int(line.split('\t')[3]) - int(line.split('\t')[2])
    return ref_len / query_len


def main():
    stat = {}
    for file in glob('./bed/*.out'):
        # species = file.split('/')[-1].split('.')[0].rstrip('_genome')
        species = re.search(r'.\/bed\/(\S+).out', file)[1]
        with open(file, mode='r') as f:
            for line in f.readlines():
                if line.startswith('#') or line.startswith('\n'):
                    continue
                stat.setdefault(line.split('\t')[0], {})
                stat[line.split('\t')[0]].setdefault(species, 0)
    for file in glob('./bed/*.out'):
        species = re.search(r'.\/bed\/(\S+).out', file)[1]
        with open(file, mode='r') as f:
            for line in f.readlines():
                if line.startswith('#') or line.startswith('\n'):
                    continue
                stat[line.split('\t')[0]][species] = coverage(line) if coverage(line) < 1 else 1
    df = pd.DataFrame(stat)
    # Convert all NaN into 0
    df = df.fillna(0)
    print(df)
    df.to_csv('./stat.csvadd', index=True, header=True)
    df = pd.read_csv('./stat.csvadd', index_col=0)
    df = df.T
    df.to_csv('./stat.T.csvadd', index=True, header=True)
    df.to_csv('./stat.T.tsvadd', index=True, header=True, sep='\t')
    print(df)

if __name__ == '__main__':
    main()
``` 
# CNE_origin
```R
a<-category#Species category
cne_yangmo_orgin<-intersect(rownames(merged_Ecne[apply(merged_Ecne[,a[!a$type %in% c("Mam","bird","Reptiles"),]$species],1,function(x){max(x)==0}),]),
	rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("Mam"),]$species],1,function(x){max(x)>0}),]))
cne_sizu_orgin<-intersect(rownames(merged_Ecne[apply(merged_Ecne[,a[!a$type %in% c("Mam","bird","Reptiles","amphibious"),]$species],1,function(x){max(x)==0}),]),rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("amphibious"),]$species],1,function(x){max(x)>0}),]))
cne_rouqiyu_orgin<-intersect(rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("wenchang","ruanguyu","yuankou","NoTeleostRayfin","Teleost"),]$species],1,function(x){max(x)==0}),]),rownames(merged_Ecne[merged_Ecne$Coelacanth>0,]))
cne_bone_orgin<-intersect(rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("wenchang","ruanguyu","yuankou"),]$species],1,function(x){max(x)==0}),]),rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("NoTeleostRayfin","Teleost"),]$species],1,function(x){max(x)>0}),]))
cne_ruangu_orgin<-rownames(merged_Ecne[apply(merged_Ecne[,a[a$type %in% c("ruanguyu","wenchang","yuankou"),]$species],1,function(x){max(x)>0}),])
```